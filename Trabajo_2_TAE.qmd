---
title: "Trabajo 2 TAE"
author: "Valentina Vanegas Castaño, Edwar Jose Londoño Correa, Andres Castrillón Velasquez, Diego Andres Chavarria Riaño, Sebastian Rendon Arteaga"
format: html
editor: visual
---

Carga del dataset

```{r}
d1<- read.csv("loan_data_2007_2014-1.csv", encoding = "UTF-8")
d2<- read.csv("loan_data_2007_2014-2.csv", encoding = "UTF-8")
d3<- read.csv("loan_data_2007_2014-3.csv", encoding = "UTF-8")
daux<- rbind(d1,d2)
data <- rbind(daux,d3)
```

Descripción de los datos

```{r}
summary(data)
```

En el resumen anterior, pudimos identificar que existen variables que todos sus valores son N/A. A continuación de examinará cuales variables son estas

```{r}
apply(X = is.na(data), MARGIN = 2, FUN = sum)

```

Con lo anterior se observa que las variables: dti_joint, open_il_6m, open_il_24m, total_bal_il, open_rv_12m, max_bal_bc, total_cu_tl, annual_inc_joint, verification_status_joint, open_acc_6m, open_il_12m, mths_since_rcnt_il, il_util, open_rv_24m, all_util, inq_fi, inq_last_12m poseen puros valores N/A, por lo que a continuación serán eliminadas del dataset.

También se borran la variables id, member_id, url, desc, zip_code, application_type, policy_code, home_ownership, ya que no nos aportarán en el modelo a construir.

```{r}
data <- subset(data,select = -c(dti_joint, open_il_6m, open_il_24m, total_bal_il, open_rv_12m, max_bal_bc, total_cu_tl, annual_inc_joint, verification_status_joint, open_acc_6m, open_il_12m, mths_since_rcnt_il, il_util, open_rv_24m, all_util, inq_fi, inq_last_12m, id, url, desc, zip_code, application_type, member_id, policy_code, home_ownership))
```

Se verifica el tipo de dato de las variables en el dataset

```{r}
sapply(data, class)
```

En el dataset tenemos variables de tipo numericas y de tipo character, para facilidad en el trabajo, se cambian las variables de tipo character a factor.

```{r}
data$term <- as.factor(data$term)
data$grade <- as.factor(data$grade)
data$sub_grade <- as.factor(data$sub_grade)
data$emp_title <- as.factor(data$emp_title)
data$emp_length <- as.factor(data$emp_length)
data$verification_status <- as.factor(data$verification_status)
data$issue_d <- as.factor(data$issue_d)
data$loan_status <- as.factor(data$loan_status)
data$pymnt_plan <- as.factor(data$pymnt_plan)
data$purpose <- as.factor(data$purpose)
data$title <- as.factor(data$title)
data$addr_state <- as.factor(data$addr_state)
data$initial_list_status <- as.factor(data$initial_list_status)
```

Se eliminan las variables addr_state, sub_grade, emp_title, issue_d, title

```{r}
data <- subset(data,select = -c(addr_state, sub_grade, emp_title, issue_d, title))
```

Se procede a separar las variables en numéricas, de cadena y de tipo fecha

```{r}
variables_num <- sapply(data, is.numeric)
data_num <- data[variables_num]
variables_cat <- sapply(data, is.factor)
data_cat <- data[variables_cat]
data_tipoFecha <- subset(data,select= c(earliest_cr_line, last_pymnt_d, next_pymnt_d, last_credit_pull_d))
```

Se analiza si existen registro duplicados

```{r}
data_duplicated <- duplicated(data)
sum(data_duplicated)
```

No existen registros duplicados.

## Análisis de Variable categóricas

Se visualiza un resume de las variables categoricas

```{r}
summary(data_cat)
```

Se separa la variable loan_status de las demás variables categóricas.

```{r}
Y<- subset(data_cat, select = loan_status)
data_cat <- subset(data_cat, select = -c(loan_status))
```

Se visualiza si las variables categóricas poseen valores N/A

```{r}
apply(X = is.na(data_cat), MARGIN = 2, FUN = sum)
```

Se observa que no hay valores N/A en las variables categóricas. A continuación, se crean las variables dummys para cada variable categorica

```{r}
library(caret)
onehotencoding <- dummyVars(~.,data = data_cat)
data_cat_dummy <- as.data.frame(predict(onehotencoding,data_cat))
```

## Análisis de Variable numéricas

```{r}
summary(data_num)
```

Se eliminan las variables mths_since_last_delinq, mths_since_last_record, mths_since_last_major_derog, tot_coll_amt, tot_cur_bal, total_rev_hi_lim, por la cantidad tan elevada de valores N/A que poseen estas variables.

```{r}
data_num <- subset(data_num, select= -c(mths_since_last_delinq, mths_since_last_record, mths_since_last_major_derog, tot_coll_amt, tot_cur_bal, total_rev_hi_lim))
```

A continuación, se completa las variables numéricas que poseean valores N/A cambiandolas por el valor de la mediana de cada atributo.

```{r}
data_num$annual_inc[is.na(data_num$annual_inc)] <- 63000
data_num$delinq_2yrs[is.na(data_num$delinq_2yrs)] <- 0.0000
data_num$inq_last_6mths[is.na(data_num$inq_last_6mths)] <- 0.0000
data_num$open_acc[is.na(data_num$open_acc)] <- 10.00
data_num$pub_rec[is.na(data_num$pub_rec)] <- 0.0000
data_num$revol_util[is.na(data_num$revol_util)] <- 57.60
data_num$total_acc[is.na(data_num$total_acc)] <- 23.00
data_num$collections_12_mths_ex_med[is.na(data_num$collections_12_mths_ex_med)] <- 0.0000
data_num$acc_now_delinq[is.na(data_num$acc_now_delinq)] <- 0.000000

```

```{r}
summary(data_num)
```

## Modificación de la variable objetivo

Se crea una función para la transformación de la variable loan_status.

```{r}
transformacion_varialeObjetivo <- function(valor){
  valor_nuevo <- ""
  if ((as.character(valor) == "Late (31-120 days)") | (as.character(valor) == "Default")){
    valor_nuevo <- "Incumple"
  }else{
    valor_nuevo <- "Cumple"
  }
  return(valor_nuevo)
}
```

Se aplica la función creada a la variable loan_status.

```{r}
Y$loan_status <- sapply(Y$loan_status,transformacion_varialeObjetivo)
```

```{r}
dplyr::count(Y, Y$loan_status, sort = TRUE)
```

Se unen los datos numéricos con los datos categóricos

```{r}
datos <- cbind(data_num,data_cat_dummy,Y)
```

## División del conjunto en entrenamiento y validación

```{r}
n_muestra <- dim(datos)[1]
p_vl <- 0.2 # proporción de datos para validación
ix_vl <- sample(n_muestra,size = round(n_muestra*p_vl),
                replace = FALSE)
datos_vl <- datos[ix_vl,]
datos_tr <- datos[-ix_vl,]
```

Hagamos una revisión de la tasa de compra en los conjuntos de entrenamiento y de validación:

-   En entrenamiento:

```{r}
proportions(table(datos_tr$loan_status))
```

-   En validación:

```{r}
proportions(table(datos_vl$loan_status))
```

## Reducción de la dimensionalidad

Estandarización

Se estandarizan las variables numéricas

```{r}
X_tr <- subset(datos_tr,select = -loan_status)
Y_tr <- datos_tr$loan_status
Y_tr <- as.factor(Y_tr)
```

```{r}
media_X_tr <- colMeans(X_tr)
sd_X_tr <- apply(X_tr,2,sd)
X_tr_std <- scale(X_tr,center = media_X_tr, scale = sd_X_tr)
```

### ACP

```{r}
#X_tr_std <- as.data.frame(X_tr_std)
ACP <- prcomp(X_tr_std, center = FALSE)
```

```{r}
summary(ACP)
```

De acuerdo con lo anterior, para conservar el 92% de la variabilidad explicada en las variables, se va a trabajar con las primeras 43 componentes principales.

```{r}
X_tr_acp <- data.frame(ACP$x[,1:4])
```

## Ajuste de un modelo de vecinos más cercanos

```{r}
library(caret)
```

```{r}
modelo_knn <- knn3(x=X_tr_acp, y = Y_tr, k = 3)
```

```{r}
predicciones_tr <- predict(modelo_knn, newdata = X_tr_acp, type = "class")
```

Matriz de confusión

```{r}
(mat_conf_tr <- table(predicciones_tr,Y_tr))
```


Seleccion del K vecinos: 


```{r}
performance_knn <- function(k,x,y){
  modelo_knn <- knn3(x=x, y = y, k = k)
  predicciones <- predict(modelo_knn, newdata = x, type = "class")
  mat_conf <- table(predicciones,y)
  accuracy <- sum(diag(mat_conf))/sum(mat_conf)
  sensibilidad <- mat_conf[2,2]/(mat_conf[2,2]+mat_conf[1,2])
  especificidad <-    mat_conf[1,1]/(mat_conf[1,1]+mat_conf[2,1])
  resultado <- list(accuracy=accuracy,
                    sensibilidad=sensibilidad,
                    especificidad=especificidad)
  return(resultado)
}
```


```{r}
k <- 2:30
performance_tr <- sapply(k,performance_knn,x = X_tr_acp, y = Y_tr)
```

```{r}
plot(k,performance_tr[1,], main = "Accuracy")
```

```{r}
plot(k,performance_tr[2,], main = "Sensibilidad")
```

```{r}
plot(k,performance_tr[3,], main = "Especificidad")
```



Ahora se busca el mejor k en validacion:

Primero hay que realizar el preprocesamiento

```{r}
X_vl <- subset(datos_vl,select = -Purchase)
Y_vl <- datos_vl$Purchase
```

```{r}
X_vl_std <- scale(X_vl,center = mediaXtr, scale = sdXtr)
```


```{r}
ACP_vl <- predict(ACP, newdata= X_vl_std)
```

```{r}
X_vl_acp <- data.frame(ACP_vl$x[,1:30])
```


Ahora si se puede realizar la busqueda del k en validacion.

```{r}
performance_knn_vl <- function(k,x,y,newx,newy){
  modelo_knn <- knn3(x=x, y = y, k = k)
  predicciones <- predict(modelo_knn, newdata = newx, type = "class")
  mat_conf <- table(predicciones,newy)
  accuracy <- sum(diag(mat_conf))/sum(mat_conf)
  sensibilidad <- mat_conf[2,2]/(mat_conf[2,2]+mat_conf[1,2])
  especificidad <-    mat_conf[1,1]/(mat_conf[1,1]+mat_conf[2,1])
  resultado <- list(accuracy=accuracy,
                    sensibilidad=sensibilidad,
                    especificidad=especificidad)
  return(resultado)
}
```


```{r}
k <- 2:30
performance_vl <- sapply(k,performance_knn_vl,x = X_tr_acp, y = Y_tr, 
                         newx = X_vl_acp, newy = Y_vl,simplify = TRUE)
```

```{r}
plot(k,performance_vl[1,], main = "Accuracy Validación")
```

```{r}
plot(k,performance_vl[2,], main = "Sensibilidad Validación")
```

```{r}
plot(k,performance_vl[3,], main = "Especificidad Validación")
```

Gráfica en el espacio ROC:

```{r}
par(pty="s")
plot(1-unlist(performance_vl[3,]),performance_vl[2,],las = 1,
     xlab = "1-Especificidad", ylab = "Sensibilidad",
     xlim = c(0,1),
     ylim = c(0,1),
     main = "ROC para el número de vecinos",
     sub = "Validación")
grid()
abline(a=0,b=1,col = "red")
```
